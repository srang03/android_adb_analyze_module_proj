### 1. 연구 개요

   #### 1.1 연구 목표

* **핵심 목표**: 선별된 6종 Android 시스템 로그의 개별적인 카메라 탐지 성능 정량적 측정 및 특화 영역 규명
* **세부 목표**: 앱 아키텍처와 로그 휘발성이 각 로그의 탐지 성능에 미치는 영향 규명
* **혁신적 접근**: 이중 분석 프레임워크를 통한 "카메라 사용" vs "카메라 촬영" 탐지 성능 분리 측정

#### 1.2 가설 구조

* **H1 (앱 아키텍처 가설)**: S1, S2, S3, S4 앱 유형별로 각 로그의 탐지 성능(F1-Score)에 통계적으로 유의미한 차이가 존재
* **H2 (휘발성 가설)**: V0, V-Time, V-Usage, V-Event 조건별로 각 로그의 탐지 성능에 통계적으로 유의미한 차이가 존재

#### 1.3 이중 분석 프레임워크

본 연구는 기존 연구의 단순한 '촬영함/안함' 구분을 넘어, **각 로그가 언제부터 무엇을 탐지하는지**를 정밀하게 규명합니다.

* **분석 1: 카메라 사용 탐지 성능**
  - Ground Truth: 촬영N=1, 촬영Y=1 (모두 카메라 앱을 사용)
  - 목적: "카메라 앱 사용"을 잘 탐지하는 로그 식별
  
* **분석 2: 카메라 촬영 탐지 성능**
  - Ground Truth: 촬영N=0, 촬영Y=1 (실제 촬영 여부)
  - 목적: "실제 촬영 행위"를 정확히 구분하는 로그 식별

### 2. 실험 설계 방법론

   #### 2.1 독립 변수 설정

**변수 1: 앱 아키텍처 유형 (4수준)**
* **S1 (직접 시스템 API 호출)**: Camera2/CameraX API를 직접 호출하는 표준 방식 (`com.sec.android.app.camera`)
* **S2 (인텐트 기반 호출)**: MediaStore.ACTION_IMAGE_CAPTURE 인텐트를 통한 위임 방식 (`com.kakao.talk`)
* **S3 (앱 내장 카메라)**: 독자적인 카메라 UI 구현 방식 (`org.telegram.messenger`)
* **S4 (특수 목적 구현)**: 무음/보안 등 특수 기능을 위한 비표준적 방식 (`com.aprogrammer.silentcamera`)

**변수 2: 로그 휘발성 조건 (4수준)**
* **V0 (즉시 수집)**: 카메라 행위 완료 직후 10초 이내 수집 (기준조건)
* **V-Time (24시간 후 수집)**: 동일한 카메라 행위에 대해 24시간 후 재수집하여 시간 경과 효과 측정
* **V-Usage (로그 버퍼 덮어쓰기 후 수집)**: 체계적 간섭 시나리오 수행 후 수집
  - 1단계: 앱 사용 (브라우저 검색, 갤러리 확인, 동영상 재생, SNS 메시지)
  - 2단계: 추가 카메라 세션 (4개 앱에서 각각 카메라 실행→촬영→종료)
* **V-Event (재부팅 후 수집)**: 시스템 재부팅 후 5분 대기 후 수집

   #### 2.2 종속 변수 설정

**주 종속 변수**: F1-Score (정밀도와 재현율의 조화 평균)
* **카메라 사용 탐지 F1-Score**: 카메라 앱 사용을 탐지하는 성능
* **카메라 촬영 탐지 F1-Score**: 실제 촬영 행위를 구분하는 성능

**보조 지표**
* **Accuracy**: 전체적 성능 파악
* **Precision**: 오탐 정도 측정
* **Recall**: 미탐 정도 측정
* **95% 신뢰구간**: 결과의 정밀도 평가

#### 2.3 실험 규모 및 구조

**전체 실험 구조**:
* **파일럿 테스트**: 32세션 (4앱 × 2케이스 × 4휘발성 × 1반복)
* **본 실험**: 160세션 (4앱 × 2케이스 × 4휘발성 × 5반복)
* **총 실험**: **192세션** (파일럿 32 + 본실험 160)

**효율적 실험 전략**:
* **V0-V-Time 연계**: V0 조건 실험 후 동일 세션에 대해 24시간 후 V-Time 자동 적용
* **V-Usage 연속성**: 간섭 시나리오의 "추가 카메라 세션"을 활용하여 2개 세션 데이터 동시 확보
* **실험 시간 최적화**: 효율적 연계 전략을 통한 실험 시간 단축

#### 2.4 표본크기 결정 방법론

**Step 1: 이론적 최소 요구사항**

**1-1. 중심극한정리 적용**
- **이론적 근거**: 표본크기가 30 이상일 때 표본평균의 분포가 정규분포에 근사하여 ANOVA의 정규성 가정을 만족한다.
- **본 연구 검증**:
  - H1 분석: 각 앱그룹(S1-S4)당 **40개** > 30개 ✓
    - 계산: 40개 = 2(촬영여부) × 4(휘발성조건) × 5(본실험반복)
    - 근거: 파일럿 테스트는 실행가능성 검증 목적으로 통계분석에서 제외
  - H2 분석: 각 휘발성그룹(V0-V-Event)당 **40개** > 30개 ✓
    - 계산: 40개 = 4(앱종류) × 2(촬영여부) × 5(본실험반복)

**1-2. 균등 표본크기의 중요성**
- **통계적 이유**:
  - 검정력 최대화: 동일한 총 표본수에서 최대 검정력 달성
  - F 분포 안정성: 불균등시 Type I 오류율 증가 위험
  - 사후검정 용이성: Tukey HSD 등의 정확한 적용
- **본 연구 설계**:
  - 모든 그룹을 **40개로 균등** 설계 (본실험 160세션 기준)
  - **본실험 160세션**을 각 분석별로 균등 배분
  - 최적의 통계적 검정력 확보

**결론**: 중심극한정리 조건 및 균등 표본크기 원칙 **완벽 충족**

**Step 2: 효과크기 추정의 현실적 한계 고려**

**2-1. 문제 상황 인식**
- **핵심 딜레마**: 실험 전에 로그별/앱별 성능 차이(효과크기)를 정확히 알 수 없음
- **구체적 문제**: 각 로그의 S1-S4 앱별, V0-V-Event 조건별 F1-Score 차이 예측 불가
- **설계 요구사항**: 표본크기를 사전에 결정해야 실험 계획 수립 가능

**2-2. 해결 방안 모색**
```
방법 1: 선행연구에서 효과크기 참조
현실: 개별 로그 성능 기반 이중 분석 연구 선행사례 부재 ❌

방법 2: 파일럿 실험으로 효과크기 추정  
현실: "파일럿도 몇 번 해야 하는가?" 순환논리 + 시간 제약 ❌

방법 3: 표준 효과크기로 보수적 설계 ✅
채택: Cohen(1988) 중간-큰 효과크기(f=0.30) 적용
```

**2-3. 보수적 추정의 논리적 근거**
- **기술적 근거**:
  - 로그별로 탐지 메커니즘이 상이함 (서비스 vs 사용패턴 vs 행위증거)
  - 앱별로 카메라 구현 방식이 다름 (API vs 인텐트 vs 내장 vs 특수)
  - 휘발성 조건별로 로그 보존 특성이 달라짐
- **학술적 근거**:
  - **Cohen(1988)**: "효과크기를 모를 때는 중간 효과크기 권장"
  - **탐색적 연구**에서 표준적으로 사용되는 보수적 접근법
  - **f=0.30**: 중간-큰 효과크기로 실용적으로 의미있는 차이의 적절한 기준

**Step 3: G*Power 계산 및 검정력 분석**

**3-1. 사전 검정력 분석 (A priori Power Analysis)**
- **목적**: "중간-큰 정도 차이가 있다면 80% 확률로 탐지할 수 있는 실험 횟수 계산"
- **분석 도구**: G*Power 3.1.9.7

**3-2. G*Power 입력값 설정**
```
분석 설정:
- Test family: F tests (ANOVA 사용)
- Statistical test: ANOVA: Fixed effects, one-way  
- Type of analysis: A priori (사전 분석)
- Effect size f: 0.30 (중간-큰 효과크기 가정)
- α error probability: 0.05 (5% 오류 허용)
- Power (1-β): 0.80 (80% 탐지력 목표)
- Number of groups: 4 (S1-S4 또는 V0-V-Event)
```

**3-3. G*Power 계산 결과**
- **그룹당 필요 표본 수**: 36개
- **총 필요 표본 수**: 144개
- **본 연구의 실제 확보**: 각 그룹당 **40개** (111% 달성)
- **예상 검정력**: **약 0.85** (목표 0.80 초과 달성)

**Step 4: 실험 설계 최적화**

**4-1. 표본크기 달성을 위한 설계**
- **목표**: 각 분석별로 그룹당 **40개 확보** (G*Power 필요 수 36개의 111%)
- **H1 분석용**: 각 앱별로 40개씩 (S1, S2, S3, S4)
- **H2 분석용**: 각 휘발성 조건별로 40개씩 (V0, V-Time, V-Usage, V-Event)

**4-2. 반복 횟수 결정**
```
계산 과정:
H1 분석 - 각 앱별 40개 달성:
40개 = 4(휘발성조건) × 2(촬영여부) × 5(본실험반복)
→ 8 × 5 = 40개 ✓

H2 분석 - 각 휘발성별 40개 달성:
40개 = 4(앱종류) × 2(촬영여부) × 5(본실험반복)
→ 8 × 5 = 40개 ✓

파일럿 테스트: 1회 (실행 가능성 검증 목적)
- 목적의 차별성: 실행 가능성 검증 vs 성능 측정
- 이론적 선별 완료: 3.3절에서 이미 6개 로그 선별됨
- 기술적 검증 중심: 수집 절차, 분석 파이프라인, 통제 방법
- 통계적 추론 불필요: 탐지 가능/불가능 확인이 목적

본 실험: 5회 (가설 검증을 위한 충분한 반복 횟수)
- 통계적 충족성: G*Power 분석 기준 상회
- 연구 윤리성: 필요충분한 데이터 확보
- 결과 신뢰성: 우발적 오류 대비 적정 안전마진
```

**4-3. 최종 실험 규모**
- **파일럿 테스트**: 4(앱) × 4(휘발성) × 2(촬영여부) × 1(반복) = **32세션**
- **본 실험**: 4(앱) × 4(휘발성) × 2(촬영여부) × 5(반복) = **160세션**  
- **총 실험**: **192세션** (파일럿 32 + 본실험 160)
- **각 그룹별 표본크기**: **40개** (G*Power 목표 36개 초과 달성)
- **실제 달성 검정력**: **약 0.85** (목표 0.80 초과 달성)
- **효율적 전략 적용으로 실제 소요 시간 단축**

### 3. 데이터 분석 및 통계 검증

#### 3.1 이중 분석 방법론

**동일한 예측 데이터 → 서로 다른 두 Ground Truth 비교**

**분석 1: 카메라 사용 탐지 성능**
```python
# Ground Truth: 촬영N과 촬영Y 모두 카메라 앱을 실행 (본실험 160세션만)
GT_usage = [1] * 160  # 본실험 160세션에서 카메라 사용 = 1

# 각 로그별 성능 측정
for log in ['media.camera', 'usagestats', 'activity', 'audio', 'media.metrics', 'vibrator_manager']:
    usage_performance = calculate_performance(GT_usage, log_predictions[log])
    # 결과: 어떤 로그가 "카메라 앱 사용"을 잘 탐지하는가?
```

**분석 2: 카메라 촬영 탐지 성능**
```python
# Ground Truth: 실제 촬영 여부 (본실험 160세션만)
# 본실험 160세션: (촬영N 80개 + 촬영Y 80개)
GT_capture = [0]*80 + [1]*80  # 촬영N=0, 촬영Y=1

# 동일한 예측값을 다른 GT와 비교 (파일럿은 별도 처리)
for log in ['media.camera', 'usagestats', 'activity', 'audio', 'media.metrics', 'vibrator_manager']:
    capture_performance = calculate_performance(GT_capture, log_predictions[log])
    # 결과: 어떤 로그가 "실제 촬영 행위"를 정확히 구분하는가?
```

**파일럿 테스트 데이터 활용**:
- **목적**: 실행가능성 검증 및 로그 특성 예비 파악
- **처리**: 통계적 가설검증에는 미포함, 질적 분석으로 보완

#### 3.2 통계적 가설 검증

**개별 로그별 독립적 통계 분석 전략**:

**1단계: 개별 로그별 주효과 검정** (α = 0.05)
- 6개 로그 각각에 대해 독립적으로 H1, H2 가설 검증  
- 각 로그별로 4개 ANOVA (H1_usage, H1_capture, H2_usage, H2_capture)
- 총 24개의 독립적 ANOVA 분석 수행 (6로그 × 2분석 × 2가설)

**2단계: 유의미한 결과에 대한 사후분석** (α = 0.05)
- 1단계의 24개 ANOVA 중 p < 0.05인 결과에 대해서만 사후분석 수행
- Tukey HSD (H1: 앱 아키텍처 간 다중비교, 최대 12개)
- Dunnett Test (H2: V0 기준 대조 비교, 최대 12개)

**3단계: 실무적 의미 평가**
- **효과크기 중심 해석**: Cohen's f ≥ 0.4를 실무적 중요성 기준
- **95% 신뢰구간**: 결과의 정밀도 평가  
- **실용적 등가성**: Cohen's f < 0.2인 경우 실무적 차이 없음으로 해석
- **다중비교 보정**: 24개 분석에 대한 Type I 오류 통제 고려

### 4. 연구 윤리 및 품질 보장

#### 4.1 사전 설계의 고정 원칙
- **연구 윤리**: 결과를 보고 표본크기를 임의 조정하지 않음
- **투명성**: 효과크기 추정의 한계를 명시적으로 인정
- **일관성**: 사전에 계획된 **192세션**을 변경하지 않고 완료

#### 4.2 데이터 무결성 보장
- **SHA-256 해시값**: 모든 로그 파일의 무결성 검증
- **세션 통합 해시**: 세션별 모든 로그의 통합 무결성 해시 생성
- **메타데이터 검증**: 실험 조건, 타임스탬프 등 세션 메타데이터 해시 검증
- **분석 전 검증**: 데이터 분석 수행 전 모든 파일의 무결성 자동 검증

#### 4.3 사후 검정력 분석 계획
- **실험 완료 후**: 실제 달성된 효과크기 계산
- **검정력 재계산**: 실제 검정력이 목표치(0.80)를 달성했는지 검증
- **가정 평가**: 보수적 추정(f=0.30)의 적절성 평가

#### 4.4 예상 결과별 대응방안
```
시나리오 1: 유의한 결과 + 높은 검정력 → 가설 지지, 설계 적절성 입증
시나리오 2: 유의한 결과 + 낮은 검정력 → 가설 지지, but 우연 가능성 존재  
시나리오 3: 비유의한 결과 + 높은 검정력 → 실제로 차이 없음
시나리오 4: 비유의한 결과 + 낮은 검정력 → 표본부족, 향후연구 제안

모든 경우에 대해 투명하게 보고하며, 한계점을 솔직히 인정
```

### 5. 연구의 혁신적 기여

#### 5.1 방법론적 혁신
- **이중 분석 프레임워크**: 카메라 사용 vs 촬영 탐지 분리 측정
- **개별 로그별 독립적 분석**: 6개 로그 각각의 특화 성능 정밀 측정 (총 24개 ANOVA)
- **효율적 실험 전략**: V0-V-Time 연계, V-Usage 연속성을 통한 실험 최적화
- **체계적 휘발성 분석**: 구체적 간섭 시나리오와 동일 세션 24시간 후 재분석

#### 5.2 실무적 가치
- **로그별 특화 영역 규명**: "언제 어떤 로그를 우선 확인해야 하는가?"
- **휘발성 조건별 성능 변화**: 현실적 포렌식 환경에서의 탐지 한계와 가능성
- **과학적 가이드라인 제공**: 실증 데이터 기반 구체적 로그 선택 지침

#### 5.3 탐색적 연구로서의 의의
- **연구 특성**: 로그 중심 개별 성능 분석이라는 새로운 방법론 제시
- **기대효과**: 향후 연구를 위한 정확한 효과크기 추정 기반 마련
- **학술적 기여**: Android 시스템 로그의 정량적 성능 평가 체계 확립
